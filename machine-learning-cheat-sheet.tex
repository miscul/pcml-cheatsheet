\documentclass[10pt,a4paper,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\usepackage{color,graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{underscore}
\usepackage{todonotes}

% Cheatsheet style
\input{style.tex}

% Shorthands
\renewcommand{\bf}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\balpha}{\boldsymbol\alpha}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\bdelta}{\boldsymbol\delta}
\newcommand{\btheta}{\boldsymbol\theta}
\newcommand{\bPhi}{\boldsymbol\Phi}

\pdfinfo{
  /Title (Machine Learning Cheat Sheet)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Dennis Meier)
  /Subject (Machine Learning cheatsheet)
  /Keywords (machinelearning, ml, bayes, regression, classification)
}
% TODO: remove title, shift one level of sections (reorganize section?)
% TODO: formulas put vectors
% -----------------------------------------------------------------------

\begin{document}
\title{Machine Learning Cheat Sheet}

\raggedright
\footnotesize
\sffamily
\begin{multicols*}{4}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

%\begin{center}
%\Large{\underline{Machine Learning Cheat Sheet}}
%\end{center}

% ----------
\section{General}
$y = y_{true} + \epsilon$, $\hat{y} = f(\bf{\hat{x}}), \forall \bf{\hat{x}}$.

likelihood $\times$ prior = posterior (MAP) = joint (GMM) = posterior $\times$ marginal likelihood, or 
$P(A,B) = P(A|B) P(B) = P(B | A) P(A)$

log-concavity: $\log ( \sum_{k=1}^K p_k t_k) \geq \sum_{k=1}^K p_k \log t_k$

\subsection{Distributions}
Multivariate gaussian: $\mathcal{N}(\bf{X} | \bf{\mu} , \bf{\Sigma})
\implies p(\bf{X} = \bf{x}) = (2 \pi)^{-d/2} |\bf{\Sigma|}^{-1/2} \exp{[- \frac{1}{2} (\bf{x} - \bf{\mu})^T \bf{\Sigma}^{-1} (\bf{x} - \bf{\mu})]}$

Gaussian: $\mathcal{N}(X| \mu, \sigma^2)$ \\
$\implies p(X = x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{(- \frac{1}{2} ( \frac{x - \mu}{\sigma} )^2)}$

Poisson: $\mathcal{P}(X| \lambda)
\implies p(X = k) = \frac{\lambda ^ k}{k!} \exp{(- \lambda)}$

Exponential family ($\frac{h(y)}{Z}$ partition fct, $\eta$ natural parameter, $\phi(y)$ sufficient stat, $A(\eta)$ log-partition fct):
$p(y|\eta) := \frac{h(y)}{Z} \exp[\eta^T \phi(y) - A(\eta)], \frac{\partial A(\eta)}{\partial \eta} = E[\phi(\eta)], \frac{\partial^2 A(\eta)}{\partial \eta^2} = \Var[\phi(\eta)]$

\subsection{Properties}
Matrix multiplication complexity ($n \times m$ and $m \times p$): $\mathcal{O}(nmp)$, inversion: $\mathcal{O}(n^3)$

\textbf{Bayes rule}: $p(A, B) = p(A|B) p(B) = p(B|A) p(A)$

A matrix $\bf{M}$ is \textbf{positive semidefinite} $\iff \forall$ nonzero vector $\bf{a}$, we have $\bf{a}^T \bf{M} \bf{a} \geq 0$

If $\bf{V}$ symmetric positive definite, then for all $\bf{P} \neq 0$, $\bf{P^T V P}$ is positive semi-definite (and even positive definite if $\bf{P}$ is not singular).

Fisher: $I(\beta) = -\E\biggl[\frac{\partial^2 \log p(x|\beta)}{\partial\beta^2}\biggr]$

\textbf{Jensen's inequality} applied to $\log$: $\log( \mathbb{E}[X] ) \geq \mathbb{E}[\log(X)]
\implies \log ( \sum_x x \cdot p(x) ) \geq \sum_x p(x) \log(x)$

\textbf{Matrix inversion lemma}: $(\bf{PQ} + \bf{I}_N)^{-1} \bf{P} = \bf{P}(\bf{QP} + \bf{I}_M)^{-1}$

Useful derivatives: $\frac{\partial \bf{x^T B x}}{\partial \bf{x}} = (\bf{B + B^T}) \bf{x}$
$\frac{\partial}{\partial x} \log[1+\exp{x}] = \sigma(x)$

Marginal and Conditional Gaussians:
$p(\bf{x}) = \mathcal{N}(\bf{x} | \boldsymbol\mu, \boldsymbol\Lambda^-1)$ 

$p(\bf{y}|\bf{x}) = \mathcal{N}(\bf{y} | \bf{Ax + b, L}^-1)
\Rightarrow
p(\bf{y}) = \mathcal{N}(\bf{y} | \bf{A} \boldsymbol\mu + \bf{b}, \bf{L}^-1 + \bf{A} \boldsymbol\Lambda^-1 \bf{A}^T)$

$p(\bf{x}|\bf{y}) = \mathcal{N}(\bf{x} |\boldsymbol\Sigma \{ \bf{A^T L(y - b) + \Lambda \mu} \}, \boldsymbol\Sigma)
\text{ where } \boldsymbol\Sigma = (\boldsymbol\Lambda + \bf{A^T L A})^{-1}
$

% ----------
\section{Optimization methods}

\subsection{Grid Search}
Simply try values for all parameters at regular intervals.
Complexity: $\mathcal{O}(M^D N D)$, where $M$ is the number of values tried in each dimension.

\subsection{Gradient Descent}
Update rule: $\bbeta^{(k+1)} = \bbeta^{(k)} - \alpha \frac{\partial \mathcal{L}(\bbeta^{(k)})}{\partial \bbeta}$

Complexity: $\mathcal{O}(I N D)$ where $I$ is the number of iterations we take.

$\alpha$ is a parameter that needs to be chosen carefully.

The gradient for MSE comes out as:
$\frac{\partial \mathcal{L}}{\partial \bbeta} = - \frac{1}{N} \tilde{X}^T ( \boldsymbol y - \tilde{X} \bbeta )$

Stochastic GD: when $N$ large, subsample and have sequence of $\alpha^{(k)}$. Ex: $\alpha^{(k)} = 1 / (1 + k)^r$, $r \in (0.5, 1)$. Robbins-Monroe condition: $\sum_{k=1}^\infty \alpha^{(k)} = \infty$, $\sum_{k=1}^\infty (\alpha^{(k)})^2 < \infty$

\subsection{Newton's method}
It uses second order derivatives information to converge faster (we approximate the objective function by a quadratic function rather than a line).

General rule: $\bbeta^{(k+1)} = \bbeta^{(k)} - \alpha \bf{H_k^{-1}} \frac{\partial \mathcal{L}(\bbeta^{(k)})}{\partial \bbeta}$\\
where $\bf{H_k}$ is the $D \times D$ Hessian at step $k$: $\bf{H_k} = \frac{\partial^2 \mathcal{L}(\bbeta^{(k)})}{\partial \bbeta^2}$

Newton's method is equivalent to solving many least-squares problems.

Complexity: $\mathcal{O}(I N D^2 + I D^3)$, with the $D^3$ cost coming from the inversion of $\bf{H_k}$.

IRLS: $\alpha_k = 1$ and as a sequence of least-squares $\bbeta^{(k+1)} = (\tilde{\bf{X}}^T \bf{S}_k \tilde{\bf{X}})^{-1} \tilde{\bf{X}}^T \bf{S}_k 
[\tilde{\bf{X}} \bbeta^{(k)} + \bf{S}_k^{-1} (\bf{y} - \bf{\sigma}_k)]$

\subsection{Expectation-Maximization}
EM is method to find MLE/MAP estimator if some of the data is missing (e.g. latent variables) and no closed form for likelihood is possible. $\btheta^{(i+1)} = \argmax_{\theta} \sum _{n=1}^N \mathbb{E}_{p(r_n | \bf{x}_n , \btheta^{(i)})} [\log p(\bf{x}_n , r_n | \btheta)]$.

\textbf{GMM} $p_{kn}^{(i)} = p(r_n | \bf{x}_n, \btheta^{(i)})$ being posterior distribution of latent variable, iteratively until convergence:

\begin{itemize}
\item \textbf{E-step} get lower bound to cost s.t. tight at previous $\btheta^{(i)}$ (using log-concavity): $p_{kn}^{(i)} = \frac{\pi_k^{(i)} \mathcal{N}(\bf{x}_n | \bf{\mu}_k^{(i)}, \bf{\Sigma}_k^{(i)})}{\sum_{k=1}^K \pi_k^{(i)} \mathcal{N}(\bf{x}_n|\bf{\mu}_k^{(i)}, \bf{\Sigma}_k^{(i)})}$ 
and cost: $\mathcal{L}(\btheta^{(i)}) = \sum_{n=1}^N \log \sum_{k=1}^K \pi_k^{(i)} \mathcal{N}(\bf{x}_n | \bf{\mu}_k^{(i)}, \bf{\Sigma}_k^{(i)})$

\item \textbf{M-step} update: $
\max_{\btheta} \sum_{n=1}^N \sum_{k=1}^K p_{kn}^{(i)} [\log \pi_k + \log \mathcal{N} ( \bf{x}_n | \bf{\mu}_k, \bf{\Sigma}_k)], 
\bf{\mu}_k^{(i+1)} = \frac{\sum_{n=1}^N p_{kn}^{(i)} \bf{x}_n}{p_{kn}^{(i)}},
\bf{\Sigma}_k^{(i+1)} = \frac{\sum_{n=1}^N p_{kn}^{(i)} (\bf{x}_n - \bf{\mu}_k^{(i+1)})(\bf{x}_n - \bf{\mu}_k^{(i+1)})^T }{p_{kn}^{(i)}},
\pi_k^{(i+1)} = \frac{1}{N} \sum_{n=1}^N p_{kn}^{(i)}
$

\end{itemize}

% ----------
\section{Regression}
Simple linear regression: $y_n \approx \beta_0 + \beta_1 x_{n1}$

Multiple linear regression: $y_n \approx f(\bf{x}_n) := \beta_0 + \beta_1 x_{n1} + \beta_2 x_{n2} + ... + \beta_D x_{nD}$

Nonlinear basis function (polynomial, gaussian, ...): $y_n \approx f(\bf{x}_n) := \beta_0 + \beta_1 \phi_1(\bf{x_n}) + ... + \beta_D \phi_D(\bf{x_n})$

L2 regularizer: $\frac{\lambda}{2N}\sum_{j=1}^M\beta_j^2$ (not $\beta_0$).

\subsection{Linear basis function model}
We can create more complex models while staying in the linear framework by transforming the inputs $X$ of dimensionality $D$ through a function $\phi : D \rightarrow M$.

$y_n = \beta_0 + \sum_{i=1}^{M} \beta_i \phi_i(\bf{x_n}) =  \bf{\tilde\phi^T}(\bf{x}^T_n) \bbeta$.
The optimal $\bbeta$ can be computed in closed form by $\bbeta = ( \tilde{\bPhi}^T \tilde{\bPhi})^{-1} \tilde{\bPhi}^T y$ where $\tilde{\bPhi}$ is a matrix with N rows and the n-th row is $[1, \phi_1(\bf{x}_n),  ...,  \phi_M(\bf{x}_n)]$. But note this requires $\tilde{\bPhi}^T \tilde{\bPhi}$ to be invertible (well-conditionned: $\tilde\bPhi$ full column-rank).

Ridge regression: $\bbeta_{ridge} = ( \tilde{\boldsymbol\Phi}^T \tilde{\boldsymbol\Phi} + \lambda \boldsymbol I)^{-1} \tilde{\boldsymbol\Phi}^T \boldsymbol y$ ($\boldsymbol I$ is $M+1 \times M+1$ and $=0$ on top left). Improves condition number of Gram matrix by lifting eigenvalues.

MAP: $\bbeta_{ridge} = \arg \max_\beta \log[\prod\limits_{n=1}^N \mathcal{N}(y_n | \bf{x}_n^T\bbeta, \lambda) \times \mathcal{N}(\bbeta|0,\bf{I})]$

\subsection{Cost functions}
%\begin{colfig}
%\centering
%\includegraphics[width=\linewidth]{images/error-functions.png}
%\end{colfig}

Cost function / Loss: $\mathcal{L}(\bbeta) = \mathcal{L}(\mathcal{D},\bbeta)$

Mean square error (MSE): $\frac{1}{2N} \sum_{n=1}^{N}\left[y_n-f(\bf{x}_i) \right]^2$

MSE matrix formulation: $\frac{1}{2N} (\bf{y - X \bbeta})^T (\bf{y - X \bbeta})$

Mean absolute error (MAE): $\frac{1}{2N} \sum_{n=1}^{N}\left | y_n-f(\bf{x}_i) \right |$

Huber loss: $\mathcal{L}_\delta (a) = \begin{cases}
\frac{1}{2}{a^2}                   & \text{for } |a| \le \delta, \\
\delta (|a| - \frac{1}{2}\delta ), & \text{otherwise.}
\end{cases}$

Root mean square error (RMSE): $\sqrt{2 * \text{MSE}}$

Epsilon insensitive (``hinge loss'', used for SVM):
$\mathcal{L}_{\epsilon}(y, \hat{y}) = \begin{cases}
0                   & \text{if } |y - \hat y| \le \epsilon, \\
|y - \hat y| - \epsilon, & \text{otherwise.}
\end{cases}$

% ----------
\section{Least squares}
Complexity: $\mathcal{O}(ND^2 + D^3)$
The gradient of the MSE set to zero is called the normal equation:
$  \bf{X}^T \bf{X} \bbeta - \bf{X}^T \bf{y} = 0$.

Geometric interpretation: choosing a vector in the span of $\tilde{X}$. $\tilde{X}$ needs full rank ($=D$) $\Rightarrow$ dim. of null space 0 $\Rightarrow$ Gram matrix positive definite $\Rightarrow$ invertible.

We can solve this for $\bbeta$ directly (by matrix manipulation)
$\bbeta = ( \bf{\tilde{X}}^T \bf{\tilde{X}} )^{-1} \bf{\tilde{X}}^T \bf{y}$

Doesn't work when $\tilde{X}$ is rank deficient ($D>N$) or matrix is \textbf{ill-conditioned} (columns (nearly) colinear, rounding can cause problems). For ill-conditioned matrices we may use partial pivoting (pivoting on the largest variable available in column).

% ----------
\section{Classification}
Logistic Function: $\sigma(t) = \frac{exp(t)}{1+exp(t)}$\\
Derivative: $\frac{ \partial\sigma(t) }{ \partial t } = \sigma(t)[ 1 - \sigma(t) ]$

Classification with linear regression: Use $y = 0$ as class $\mathcal{C}_1$
and $y = 1$ as class $\mathcal{C}_2$ and then decide a newly estimated $y$ belongs
to $\mathcal{C}_1$ if $y < 0.5$.

$k$-Nearest Neighbor: $f_k(\bf{x}_*) = k^{-1} \sum_{\bf{x}_n \in nbh_k (\bf{x}_*)}$, $nbh_k (\bf{x})$ $k$ closest points of $\bf{x}$.

\subsection{Logistic Regression}
$\mathcal{L}_{mle}(\bbeta) = \sum\limits_{n=1}^N y_n \log \sigma(\tilde{\bf{x}}_n^T \bbeta) + (1-y_n) \log[1-\sigma(\tilde{\bf{x}}_n^T \bbeta)]
= \sum\limits_{n=1}^N y_n \tilde{\bf{x}}_n^T \bbeta - \log[1 + \exp(\tilde{\bf{x}}_n^T \bbeta)]$

$\frac{ \partial\mathcal{L}(\bbeta) }{ \partial \bbeta } = \tilde{\bf{X}}^T [\sigma(\tilde{\bf{X}} \bbeta) - \bf{y}]$

$\bf{H}(\bbeta) = \bf{\tilde{X}^T S \tilde{X}}, \bf{S}_{nn} = \sigma(\bf{\tilde{x}}_n^T \bbeta) [1 - \sigma(\bf{\tilde{x}}_n^T \bbeta)]$ (diag)

There's no closed form, we can use gradient descent.

Linearly separable problem has many solutions $\Rightarrow$ solved by L2 regularization (penalized log. reg.): $\min\limits_\beta -\sum\limits_{n=1}^N \log p(y_n | \bf{x}_n^T, \bbeta) + \lambda \sum\limits_{d=1}^D\beta^2_d$ or $\bf{X^T S X} + \lambda \bf{I}_{D+1}$ (top-left = 0).

\subsection{Generalized linear model}
The GLM consists of three elements: \textbf{1)} a probability distribution from the exponential family; \textbf{2)} a linear predictor $\hat y = \bf{X} \bbeta$; \textbf{3)} a link function g such that $E(\phi(y)) = \mu = g^{-1}(\eta)$.

In a GLM each outcome of the dependent variables $y$ is assumed to be generated from a particular distribution in the exponential family, e.g. normal, binomial, Poisson, gamma, etc.

ML cost $\min \mathcal{L}(\bbeta) = - \sum_{n=1}^N \log p(y_n | \eta_n)$, $p$ exp distribution, natural parameters $\eta_n = \tilde{\bf{x}}_n^T \bbeta$. Normal equation: $\frac{\partial \mathcal{L}}{\partial \bbeta} = \tilde{\bf{X}}^T [ \bf{g}^{-1}(\eta_n) - \phi(\bf{y})]$.


\subsection{Cost functions}
RMSE: $\sqrt{\frac{1}{N} \sum_{n=1}^{N}\left[y_n- \hat{p_n} \right]^2}$

0-1 Loss: $ \frac{1}{N} \sum_{n=1}^{N} \delta(y_n, \hat{y_n})$

Log-Loss: $- \frac{1}{N}  \sum_{n=1}^{N} y_n \log(\hat{p_n}) + (1-y_n) \log(1-\hat{p_n})$


% ----------
\section{Probabilistic framework}
The Likelihood Function maps the model parameters to the probability distribution of $\bf{y}$:
$\mathcal{L}_{lik}\colon \text{parameter space} \to [0;1]\quad  \bbeta \mapsto p(\bf{y} \mid  \bbeta)$
An underlying $p$ is assumed before. If the observed $y$ are IID, $p(\bf{y} \mid \bbeta) = \prod_n p(y_n \mid \bbeta)$.

$\mathcal{L}_{lik}$ can be viewed as just another cost function. Maximum likelihood then simply chooses the parameters $\bbeta$ such that observed data is most likely. $\bbeta = \argmax_{\text{all} \bbeta} L(\bbeta)$

Assuming different $p$ is basically what makes this so flexible. We can chose e.g.:

\begin{tabular}{ l  l }
  \hline
  Gaussian $p$ & $\mathcal{L}_{lik} \widehat{=} -\mathcal{L}_{MSE}$ \\
  Laplace $p$  & $\mathcal{L}_{lik} \widehat{=} -\mathcal{L}_{MAE}$ \\
  \hline
\end{tabular}

It is a sample approximation of the expected likelihood:
$\mathcal{L}_{lik}(\bbeta) \approx E_y[ p(y \mid \bbeta) ]$.
It is consistent (more data points $\Rightarrow$ converges in probability) and efficient (in best possible manner, achieves Cramor-Rao lower bound: Cov($\bbeta_{mle}$)$=\bf{I^{-1}}(\bbeta_{true})$, $\bf{I}$ is Fisher).

\subsection{Bayesian methods}
Bayes rule: $p(A, B) = p(A|B) p(B) = p(B|A) p(A)$

The \textbf{prior} $p(\bf{f}|\bf{X})$ encodes our prior belief about the ``true'' model $\bf{f}$. The \textbf{likelihood} $p(\bf{y}|\bf{f})$ measures the probability of our (possibly noisy) observations given the prior.

Least-squares tries to find model parameters $\bbeta$ which maximize the likelihood. Ridge regression maximizes the \textbf{posterior} $p(\bbeta|\bf{y})$

\subsection{Bayesian networks}
We can use a Directed Acyclic Graph (DAG) to define a joint distribution of events. For example, we can express the relationship between \textit{latent factors} (possible ``causes'') $z_i$ and \textit{observations} (results) $y_i$:

\begin{colfig}
  \centering
y  \includegraphics[height=1.5cm]{images/bayesian-network.png}
\end{colfig}

This example can be factorized as follows:
$p(y_1, y_2, z_1, z_2, z_3) = p(y_1 | z_1, z_2) p(y_2 | z_2, z_3) p(z_1) p(z_2) p(z_3)$

We can then obtain the distribution over latent factors ($z_i$) by marginalizing over the unknown variables:
$p(z_1, z_2, z_3 | y_1, y_2) = \frac{\text{joint}}{p(y_1, y_2)}$\\
$\implies p(z_1 | y_1, y_2) = \sum_{z_2, z_3} \frac{\text{joint}}{p(y_1, y_2)}$

\subsection{Belief propagation}
Belief propagation is a message-passing based algorithm used to compute desired marginals (e.g. $p(z_1 | y_1, y_2)$) efficiently. It leverages the factorized expression of the joint. Messages passed:

$m_{z_i \rightarrow y_j} = p(z_i) \Pi(\text{messages received except from } y_j)$

$m_{y_j \rightarrow z_i} = \sum_{z_k \neq z_i} p(y_j | z_k) \Pi(\text{messages received except from } z_i)$

% ----------
\section{Kernel methods}
Kernels are another way to encode our prior believe about the relation of outputs to inputs. A kernel defines a distance measure, or ``similarity'' of two vectors. We define:
$(\bf{K})_{i,j} = \kappa(\bf{x_i}, \bf{x_j}) = \vec \phi(\bf{x_i})^T \vec \phi(\bf{x_j})$.

The $\phi$ are not that important in the end, because we only use the Kernel as is. Sometimes it's even impossible to write them down explicitly.

\begin{tabular}{ l | l }
  \hline
  Linear     & $\kappa(\bf{x_i}, \bf{x_j}) = \bf{x_i}^T \bf{x_j}$ \\
  \hline
  Polynomial & $\kappa(\bf{x_i}, \bf{x_j}) = (\bf{x_i}^T \bf{x_j} + c)^d$ \\
  \hline
  RBF        & $\kappa(\bf{x_i}, \bf{x_j}) = \exp\left(-\frac{||\bf{x_i} - \bf{x_j}||^2}{2\sigma^2}\right)$ \\
  \hline
\end{tabular}

Properties of a Kernel: $\bf{1)}$ $\bf{K}$ should be symmetric: $\bf{K}^T = \bf{K}$; $\bf{2)}$ $\bf{K}$ should be positive semi-definite: $\forall$ nonzero vector $\bf{a}, \bf{a}^T \bf{K} \bf{a} \geq 0$.

\subsection{Primal vs. Dual}
Instead of working in the \textbf{column space} of our data, we can work in the \textbf{row space}:
$\bf{\hat{y} = X \bbeta = X X^T \balpha = K \balpha}$
where $\bbeta \in \mathbb{R}^D$ and $\balpha \in \mathbb{R}^N$
and (like magic) $\bf{K}$ shows up, the Kernel Matrix.

\textbf{Representer Theorem}: For any $\bbeta$ minimizing
$\min_\beta \sum_{n=1}^N \mathcal{L}(y_n, \bf{x}_n^T \bbeta) + \sum_{d=1}^D \lambda \beta_d^2$
there exists an $\balpha$ such that $\bbeta = \bf{X}^T \balpha$.

When we have an explicit vector formulation of $\bbeta$, we can use the matrix inversion lemma to get to the dual. E.g. for ridge regression:
$\bbeta = (\bf{X}^T \bf{X}  + \lambda \bf{I}_D)^{-1} \bf{X}^T \bf{y}= \bf{X}^T \underbrace{(\bf{X X}^T + \lambda \bf{I}_N)^{-1} \bf{y}}_{\balpha}$

Getting the dual in optimization (equivalent):
$\min_{\beta} g(\beta) \leftrightarrow 
\min_{\beta} \max_{\alpha} G(\beta, \alpha) \leftrightarrow 
\max_{\alpha} \min_{\beta} G(\beta, \alpha) \leftrightarrow 
\max_{\alpha} g^*(\alpha)$

Set $\bf{G}(\balpha, \bbeta)$: $C[v_n]_+ = \max_{\alpha_n} \alpha_n v_n, \alpha_n \in [0, C]$.
\textbf{Minimax}: switch min-max OK when $G(\balpha, \bbeta)$ convex in $\bbeta$ and concave in $\balpha$ and sets are convex.
\textbf{Get dual}: switch min/max, derive, equal to 0, plug back $\bbeta$.

\subsection{Gaussian Process}
The predicting function $f$ is interpreted as a random variable with jointly gaussian prior: $\mathcal{N}(f | \bf{0}, \bf{K})$.
Defining the Kernel matrix $\bf{K} = \kappa(\bf{X}, \bf{X})$ defines the prior. The key idea is, that if $\bf{x}_i$ and $\bf{x_j}$ are
deemed by the kernel to be similar, then we expect the output of $f$ at those points to be similar, too.

We can sample functions from this random variable $f$ and we can use prior + measurements to generate predictions.

If we have measurements $\bf{y}$ available, we get a joint distribution with the $\bf{\hat{y}}$ to be predicted:

$
\begin{bmatrix}
  \bf{y} \\
  \bf{\hat{y}}
\end{bmatrix}
=
\mathcal{N} \left(
  \bf{0},
  \begin{bmatrix}
    \kappa(\bf{X}, \bf{X}) + \sigma_n^2 I  & \kappa(\bf{X}, \bf{\hat{X}}) \\
    \kappa(\bf{\hat{X}}, \bf{X}),          & \kappa(\bf{\hat{X}}, \bf{\hat{X}})
  \end{bmatrix}
\right)
$

This can be conditioned on $\bf{y}$ to find the PDF of $\bf{\hat{y}}$. Advantage: we output our prediction as probabilities (which represent uncertainty).

% ----------
\section{Neural Networks}
A feed forward Neural Network is organized in $K$ layers, each layer with $M^{(k)}$ hidden units $z_i^{(k)}$. Activations $a_i^{(k)}$ are computed as the linear combination of the previous layer's terms, with weights $\bbeta^{(k)}$ (one $M^{(k-1)} \times 1$ vector of weights for each of the $M^{(k)}$ activations). Activations are then passed through a (possibly nonlinear) function $h$ to compute the hidden unit $z_i^{(k)}$.

$\bf{x}_n \xrightarrow{\bbeta_i^{(1)}} a_i^{(1)} \xrightarrow{h} z_i^{(1)} \xrightarrow{\bbeta^{(2)}} \dots \bf{z}^{(K)} = \bf{y}_n$

\subsection{Backpropagation}
It's an algorithm which computes the gradient of the cost $\mathcal{L}$ w.r.t. the parameters $\bbeta^{(k)}$.

Forward pass: compute $a_i$, $z_i$ and $\bf{y}_n$ from $\bf{x}_n$.

Backward pass: work out derivatives from outputs to the target $\bbeta_i^{(k)}$. Using the chain rule:\\
$\bdelta^{(k-1)} = \frac{\partial \mathcal{L}}{\partial \bf{a}^{(k-1)}} = diag[ h'(\bf{a}^{(k-1)}) ] \bf{B^{(k)T}} \bdelta^{(k)}$\\
$\frac{\partial \mathcal{L}}{\partial \bf{B}^{(1)}} = \bdelta^{(1)} \bf{x}^T$\\
$\frac{\partial \mathcal{L}}{\partial \bf{B}^{(k)}} = \bdelta^{(k)} \bf{z}^{(k)T}$

\subsection{Regularization}
NN are not \textit{identifiable} (existence of many local optima), therefore the maximum likelihood estimator is not \textit{consistent}.

NN are universal density estimators, and thus prone to severe overfitting. Techniques used to reduce overfitting include early stopping (stop optimizing when test error starts increasing) and ``weight decay'' (i.e. $L_2$ regularization).

% ----------
\section{Support Vector Machines}
Search for the hyperplane separating the data such that the gap (margin $M = \frac{1}{||\beta||}$) is biggest.
It minimizes the following hinge loss cost function:

$\mathcal{L}_{SVM} (\bbeta)= \sum_{n=1}^N C[1 - y_n \tilde\phi_n^T \bbeta]_{+} + \frac{1}{2} \sum_{j=1}^M \beta_j^2$, with $[t]_{+} = \max(0, t)$ (or with $\lambda$ on $2^{nd}$ term). 
Convex but not differentiable. In the dual, the same problem can be formulated as:
$\max_{\alpha \in [0; C]^N} \balpha^T \bf{1} - \frac{1}{2} \balpha^T \bf{Y K Y} \balpha , \quad \balpha^T \bf{y} = 0$

$\balpha$ is sparse and non-zero only for data instrumental in determining decision boundaries. $\alpha_n$ is the slope of lines lower bound of Hinge. 3 kinds support vectors $\tilde{\phi}_n$: $\bf{1)}$ Not a SV (outside margin, $\alpha_n = 0$); $\bf{2)}$ essential SV (on the margin, $\alpha_n \in (0,C)$); $\bf{3)}$ bound SV (outside margin, other side, $\alpha_n = C$).

% ----------
\section{Unsupervised Learning}
\subsection{K-Means}
$\min_{\bf{r,\mu}} \mathcal{L}(\bf{r, \mu}) = 
\sum_{k=1}^K \sum_{n=1}^N r_{nk} ||\bf{x}_n - \bf{\mu}_k ||_2^2, \mu_k \in \mathbb{R}$ centroids, $r_{nk} \in \{0,1\}$  cluster assignments, $\sum_{k=1}^K r_{nk} = 1$.
\textbf{Optimization}: init $\mu_k \forall k$ and iter: $\bf{1)}$ $\bf{r}_n \forall n = 1$ if closest, $0$ o/w, given $\mu$, 
$\bf{2)}$ $\mu_k \forall k = 
\frac{\sum^N_{n=1} r_{nk} \bf{x}_n}{\sum^N_{n=1} r_{nk}}$
given $\bf{r}$.
Convergence assured. Complexity $\mathcal{O}(NKDI)$, $I =$ number of iterations.

\subsection{Gaussian Mixture Models}
$K$-Means extension where examples can belong to several elliptic clusters, using full covariance matrix $\Sigma_k$. $r_n, p(r_n=k)=\pi_k, \sum \pi_k = 1$ multinomial distribution.
\textbf{Model} $p(\bf{X}, \bf{r} | \{\bf{\mu}, \bf{\Sigma}, \bf{\pi}\} = \bf{\theta}) = \prod\limits_{n=1}^N \prod\limits_{k=1}^K [\{\mathcal{N}(\bf{x}_n | \bf{\mu}_k , \bf{\Sigma}_k ) \}^{r_{nk}}] \prod\limits_{k=1}^K [\pi_k]^{r_{nk}}$

\textbf{Marginal likelihood} (because latent variable $r_n$): $p(\bf{x}_n | \bf{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\bf{x}_n | \bf{\mu}_k, \bf{\Sigma}_k)$

\textbf{Maximum likelihood} (cost, not identifiable nor convex): $\max_{\bf{\theta}} \sum_{n=1}^N \log \sum_{k=1}^K \pi_k \mathcal{N} ( \bf{x}_n | \bf{\mu}_k , \bf{\Sigma}_k)$

To use this for clustering, we first fit this mixture and then compute the posterior $p(z_i = k | \bf{x}_i, \theta)$. This yields \textit{soft} cluster assignments.

\subsection{Matrix Factorization}

Project $\bf{x}_n$ to smaller dimension $\bf{z}_n \in \mathbb{R}^M$ to minimize reconstruction error (convex, not identifiable, [regularization]): $\min_{\bf{W, Z}} \frac{1}{2} \sum_{n=1}^N \sum_{d=1}^D (x_{dn} - \bf{w}_d^T \bf{z}_n )^2 
+ [\frac{\lambda_w}{2} \sum_{d=1}^D \bf{w}_d^T \bf{w}_d + \frac{\lambda_z}{2} \sum_{n=1}^N \bf{z}_n^T \bf{z}_n], 
\bf{X}: D \times N, \bf{W}: D \times M, \bf{Z}: N \times M$

\textbf{Alternating Least-Squares} $\mathcal{O}(DM^2 + M^3) (D \gg M)$: $\bf{Z}^T \leftarrow (\bf{W}^T \bf{W} + \lambda_z \bf{I}_M )^{-1} \bf{W}^T \bf{X},
\bf{W}^T \leftarrow (\bf{Z}^T \bf{Z} + \lambda_w \bf{I}_M )^{-1} \bf{Z}^T \bf{X}^T$

% ----------
\section{PCA}
Find the eigenvectors of the covariance matrix $\bf{X^T X}$ of the data. These form an orthonormal basis $\{ \bf{w}_1, ..., \bf{w}_N\}$ for the data in the directions that have highest variance.
One can then use the first $L < D$ vectors to rebuild the data: $\bf{\hat{x}}_i = \bf{W} \bf{z}_i = \bf{W} \bf{W}^T \bf{x}_i$, with $\bf{W} = \begin{bmatrix} \bf{w}_1 ; ... ; \bf{w}_L \end{bmatrix}$.
This minimizes mean square error $\frac{1}{N} \sum_{i=1}^N \bf{x}_i - \bf{\hat{x}}_i^2$.

\section{SVD}
The same as with PCA, we can do with SVD:
\begin{colfig}
  \centering
  \includegraphics[width=\linewidth]{images/svd.png}
\end{colfig}

The singular vals of a $N \times D$ matrix $\bf{X}$ are the square roots of the eigenvalues of the $D \times D$ matrix $\bf{X^T X}$
% ----------
\section{Concepts}

\subsection{Convexity}
f is called convex f: $\forall x_1, x_2 \in X, \forall t \in [0, 1]: \qquad f(tx_1+(1-t)x_2)\leq t f(x_1)+(1-t)f(x_2).$

Sum of two convex functions is convex. Composition of a convex function with a convex, nondecreasing function is convex. Linear, exponential and $\log(\sum \exp)$ functions are convex.

\subsection{Cross-Validation}
$K$-Fold cross-validation: split training data in $K$ splits, choose $1$ as test, $K-1$ as training, repeat and average test and training error. We can get estimate of generalization error, assess quality of model, get best parameters.

\subsection{Bias-Variance Decomposition}
\begin{colfig}
  \centering
  \includegraphics[width=\linewidth]{images/bias-variance.png}
\end{colfig}

In-sample error: error of one test sample. Test error: average over test data of in-sample error. Expected test error $\overline{teErr}$: average over all training data of test error. 

Bias-variance comes directly out of the test error ($y_{true}=E[y]:$ true model, $(x,y = y_{true} + \epsilon):$ test pair observation, $\hat{y}=f(\bf{x}):$ test predictions):
 \begin{align*}
 \overline{teErr}
 &= E[(\text{observation} - \text{prediction})^2] = E[(y - \hat{y})^2] \\
 &= E[(y - y_{true} + y_{true} - \hat{y})^2] \\
 &=\underbrace{E[(y - y_{true})^2]}_{\text{var of measurement}} + E[(y_{true} - \hat{y})^2] \\
 &=\sigma^2 + E[(y_{true} - E[\hat{y}] + E[\hat{y}] - \hat{y})^2] \\
 &=\sigma^2 + \underbrace{E[(y_{true} - E[\hat{y}])^2]}_{\text{pred bias}^2} +\underbrace{E[(E[\hat{y}] - \hat{y})^2]}_{\text{pred variance}}
\end{align*}

\begin{tabular}{ l || c | c }
                          & bias & variance \\
  \hline
  regularization          & +    & - \\
  choose simpler model    & +    & - \\
  more data               & -    & \\
  \hline
\end{tabular}

\subsection{Identifiability}
We say that a statistical model $\mathcal{P} = \{P_\theta: \theta \in \Theta\}$ is identifiable if the mapping $\theta \mapsto P_\theta$ is one-to-one:
$P_{\theta_1}=P_{\theta_2} \quad\Rightarrow\quad \theta_1=\theta_2 \quad\ \text{for all } \theta_1,\theta_2\in\Theta.$

A non-identifiable model will typically have many local optima yielding the same cost, e.g. $\mathcal{L}(W, Z) = \mathcal{L}(aW, \frac{1}{a} Z)$

\subsection{Curse of dimensionality}
Dimensionality increase $\Rightarrow$ all points spread $\Rightarrow$ NN choice becomes random. 
In high dimension, data only covers a tiny fraction of the input space, making generalization difficult and increasing bias and variance.
Expected edge length $e_D(r) = r^{1/D}$: to capture $r$ of data we must cover $e_D(r)$ of input variable range $\Rightarrow$ sampling density proportional to $N^{1/D}$.



% ----- Less useful concepts


\subsection{Occam's Razor}
It states that among competing hypotheses, the one with the fewest assumptions should be selected. Other, more complicated solutions may ultimately prove correct, but in the absence of certainty, the fewer assumptions that are made, the better.

\todo[inline]{TODO: statistical goodness (robust to outliers, ...) vs. computational goodness (convex, low computational complexity, ...) tradeoff. No free lunch theorem.}

\todo[inline]{TODO: Decision Trees and Random Forests and Model averaging}

% ---------- Credits
\section{Credits}
Most material was taken from the lecture notes of \href{http://people.epfl.ch/228491}{Prof. Emtiyaz Khan}.\\
Cost functions figure from Patrick Breheny's slides.\\
Biais-variance decomposition figure from Hastie, Tibshirani, Friedman, \textit{The Elements of Statistical Learning}.
The SVD figure from Kevin P. Murphy, \textit{Machine Learning, A Probabilistic Perspective}.

% ---------- Footer
\hrule
\tiny
Rendered \today. Written by Dennis Meier and Merlin Nimier-David.
\copyright Dennis Meier. This work is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License.
To view a copy of this license, visit \href{http://creativecommons.org/licenses/by-sa/3.0/}{http://creativecommons.org/licenses/by-sa/3.0/} or
send a letter to Creative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.
\includegraphics{images/by-sa.png}

\end{multicols*}
\end{document}
