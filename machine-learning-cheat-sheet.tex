\documentclass[10pt,a4paper,landscape]{extarticle}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\usepackage{color,graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{underscore}
\usepackage{todonotes}

% Cheatsheet style
\input{style.tex}

% Shorthands
\renewcommand{\bf}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\balpha}{\boldsymbol\alpha}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\bdelta}{\boldsymbol\delta}
\newcommand{\btheta}{\boldsymbol\theta}
\newcommand{\bPhi}{\boldsymbol\Phi}

\pdfinfo{
  /Title (Machine Learning Cheat Sheet)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Dennis Meier)
  /Subject (Machine Learning cheatsheet)
  /Keywords (machinelearning, ml, bayes, regression, classification)
}
% TODO: remove title, shift one level of sections (reorganize section?)
% TODO: formulas put vectors
% -----------------------------------------------------------------------

\begin{document}
\title{Machine Learning Cheat Sheet}

\raggedright
\footnotesize
\sffamily

\setlength{\columnsep}{0.5pt}
\setlength{\columnseprule}{0.25pt}

\begin{multicols*}{4}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\premulticols}{1pt}
%\setlength{\postmulticols}{1pt}
%\setlength{\multicolsep}{10pt}
%\setlength{\columnsep}{330pt}

%\begin{center}
%\Large{\underline{Machine Learning Cheat Sheet}}
%\end{center}

% ----------
\section{General}
\subsection{Distributions}
Multivariate gaussian: $\mathcal{N}(\bf{X} | \bf{\mu} , \bf{\Sigma})
\implies p(\bf{X} = \bf{x}) = (2 \pi)^{-d/2} |\bf{\Sigma|}^{-1/2} \exp{[- \frac{1}{2} (\bf{x} - \bf{\mu})^T \bf{\Sigma}^{-1} (\bf{x} - \bf{\mu})]}$

Gaussian: $\mathcal{N}(X| \mu, \sigma^2)$ \\
$\implies p(X = x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{(- \frac{1}{2} ( \frac{x - \mu}{\sigma} )^2)}$

Poisson: $\mathcal{P}(X| \lambda)
\implies p(X = k) = \frac{\lambda ^ k}{k!} \exp{(- \lambda)}$

Exponential family ($\frac{h(y)}{Z}$ partition fct, $\eta$ natural parameter, $\phi(y)$ sufficient stat, $A(\eta)$ log-partition fct):
$p(y|\eta) := \frac{h(y)}{Z} \exp[\eta^T \phi(y) - A(\eta)], \frac{\partial A(\eta)}{\partial \eta} = E[\phi(\eta)], \frac{\partial^2 A(\eta)}{\partial \eta^2} = \Var[\phi(\eta)]$

\subsection{Properties}
$y = y_{true} + \epsilon$, $\hat{y} = f(\bf{\hat{x}}), \forall \bf{\hat{x}}$.

%TODO more clearly
likelihood $\times$ prior = posterior (MAP) = joint (GMM) = posterior $\times$ marginal likelihood, or 
$P(A,B) = P(A|B) P(B) = P(B | A) P(A)$

log-concavity: $\log ( \sum_{k=1}^K p_k t_k) \geq \sum_{k=1}^K p_k \log t_k$

Matrix multiplication complexity ($n \times m$ and $m \times p$): $\mathcal{O}(nmp)$, inversion: $\mathcal{O}(n^3)$

\textbf{Bayes rule}: $p(A, B) = p(A|B) p(B) = p(B|A) p(A)$

A matrix $\bf{M}$ is \textbf{positive semidefinite} $\iff \forall$ nonzero vector $\bf{a}$, we have $\bf{a}^T \bf{M} \bf{a} \geq 0$

If $\bf{V}$ symmetric positive definite, then for all $\bf{P} \neq 0$, $\bf{P^T V P}$ is positive semi-definite (and even positive definite if $\bf{P}$ is not singular).

Fisher: $I(\beta) = -\E\biggl[\frac{\partial^2 \log p(x|\beta)}{\partial\beta^2}\biggr]$

\textbf{Jensen's inequality} applied to $\log$: $\log( \mathbb{E}[X] ) \geq \mathbb{E}[\log(X)]
\implies \log ( \sum_x x \cdot p(x) ) \geq \sum_x p(x) \log(x)$

\textbf{Matrix inversion lemma}: $(\bf{PQ} + \bf{I}_N)^{-1} \bf{P} = \bf{P}(\bf{QP} + \bf{I}_M)^{-1}$

Useful derivatives: $\frac{\partial \bf{x^T B x}}{\partial \bf{x}} = (\bf{B + B^T}) \bf{x}$
$\frac{\partial}{\partial x} \log[1+\exp{x}] = \sigma(x)$

Marginal and Conditional Gaussians:
$p(\bf{x}) = \mathcal{N}(\bf{x} | \boldsymbol\mu, \boldsymbol\Lambda^-1)$ 

$p(\bf{y}|\bf{x}) = \mathcal{N}(\bf{y} | \bf{Ax + b, L}^-1)
\Rightarrow
p(\bf{y}) = \mathcal{N}(\bf{y} | \bf{A} \boldsymbol\mu + \bf{b}, \bf{L}^-1 + \bf{A} \boldsymbol\Lambda^-1 \bf{A}^T)$

$p(\bf{x}|\bf{y}) = \mathcal{N}(\bf{x} |\boldsymbol\Sigma \{ \bf{A^T L(y - b) + \Lambda \mu} \}, \boldsymbol\Sigma)
\text{ where } \boldsymbol\Sigma = (\boldsymbol\Lambda + \bf{A^T L A})^{-1}
$

% ----------
\section{Optimization methods}

\subsection{Grid Search}
Simply try values for all parameters at regular intervals.
Complexity: $\mathcal{O}(M^D N D)$, where $M$ is the number of values tried in each dimension.

\subsection{Gradient Descent}
Update rule: $\bbeta^{(k+1)} = \bbeta^{(k)} - \alpha \frac{\partial \mathcal{L}(\bbeta^{(k)})}{\partial \bbeta}$

Complexity: $\mathcal{O}(I N D)$ where $I$ is the number of iterations we take.

$\alpha$ is a parameter that needs to be chosen carefully.

The gradient for MSE comes out as:
$\frac{\partial \mathcal{L}}{\partial \bbeta} = - \frac{1}{N} \tilde{X}^T ( \boldsymbol y - \tilde{X} \bbeta )$

Stochastic GD: when $N$ large, subsample and have sequence of $\alpha^{(k)}$. Ex: $\alpha^{(k)} = 1 / (1 + k)^r$, $r \in (0.5, 1)$. Robbins-Monroe condition: $\sum_{k=1}^\infty \alpha^{(k)} = \infty$, $\sum_{k=1}^\infty (\alpha^{(k)})^2 < \infty$

\subsection{Newton's method}
It uses second order derivatives information to converge faster (we approximate the objective function by a quadratic function rather than a line).

General rule: $\bbeta^{(k+1)} = \bbeta^{(k)} - \alpha \bf{H_k^{-1}} \frac{\partial \mathcal{L}(\bbeta^{(k)})}{\partial \bbeta}$\\
where $\bf{H_k}$ is the $D \times D$ Hessian at step $k$: $\bf{H_k} = \frac{\partial^2 \mathcal{L}(\bbeta^{(k)})}{\partial \bbeta^2}$

Newton's method is equivalent to solving many least-squares problems.

Complexity: $\mathcal{O}(I N D^2 + I D^3)$, with the $D^3$ cost coming from the inversion of $\bf{H_k}$.

IRLS: $\alpha_k = 1$ and as a sequence of least-squares $\bbeta^{(k+1)} = (\tilde{\bf{X}}^T \bf{S}_k \tilde{\bf{X}})^{-1} \tilde{\bf{X}}^T \bf{S}_k 
[\tilde{\bf{X}} \bbeta^{(k)} + \bf{S}_k^{-1} (\bf{y} - \bf{\sigma}_k)]$

\subsection{Expectation-Maximization}
EM is method to find MLE/MAP estimator if some of the data is missing (e.g. latent variables) and no closed form for likelihood is possible. $\btheta^{(i+1)} = \argmax_{\theta} \sum _{n=1}^N \mathbb{E}_{p(r_n | \bf{x}_n , \btheta^{(i)})} [\log p(\bf{x}_n , r_n | \btheta)]$.

\textbf{GMM} $p_{kn}^{(i)} = p(r_n | \bf{x}_n, \btheta^{(i)})$ being posterior distribution of latent variable, iteratively until convergence:

$\bf{1)}$ \textbf{E-step} get lower bound to cost s.t. tight at previous $\btheta^{(i)}$ (using log-concavity): $p_{kn}^{(i)} = \frac{\pi_k^{(i)} \mathcal{N}(\bf{x}_n | \bf{\mu}_k^{(i)}, \bf{\Sigma}_k^{(i)})}{\sum_{k=1}^K \pi_k^{(i)} \mathcal{N}(\bf{x}_n|\bf{\mu}_k^{(i)}, \bf{\Sigma}_k^{(i)})}$ 
and cost: $\mathcal{L}(\btheta^{(i)}) = \sum_{n=1}^N \log \sum_{k=1}^K \pi_k^{(i)} \mathcal{N}(\bf{x}_n | \bf{\mu}_k^{(i)}, \bf{\Sigma}_k^{(i)})$

$\bf{2)}$ \textbf{M-step} update: $
\max_{\btheta} \sum_{n=1}^N \sum_{k=1}^K p_{kn}^{(i)} [\log \pi_k + \log \mathcal{N} ( \bf{x}_n | \bf{\mu}_k, \bf{\Sigma}_k)], 
\bf{\mu}_k^{(i+1)} = \frac{\sum_{n=1}^N p_{kn}^{(i)} \bf{x}_n}{p_{kn}^{(i)}},
\bf{\Sigma}_k^{(i+1)} = \frac{\sum_{n=1}^N p_{kn}^{(i)} (\bf{x}_n - \bf{\mu}_k^{(i+1)})(\bf{x}_n - \bf{\mu}_k^{(i+1)})^T }{p_{kn}^{(i)}},
\pi_k^{(i+1)} = \frac{1}{N} \sum_{n=1}^N p_{kn}^{(i)}
$

% ----------
\section{Regression}
Simple linear regression: $y_n \approx \beta_0 + \beta_1 x_{n1}$

Multiple linear regression: $y_n \approx f(\bf{x}_n) := \beta_0 + \beta_1 x_{n1} + \beta_2 x_{n2} + ... + \beta_D x_{nD}$

Nonlinear basis function (polynomial, gaussian, ...): $y_n \approx f(\bf{x}_n) := \beta_0 + \beta_1 \phi_1(\bf{x_n}) + ... + \beta_D \phi_D(\bf{x_n})$

L2 regularizer: $\frac{\lambda}{2N}\sum_{j=1}^M\beta_j^2$ (not $\beta_0$).

\subsection{Linear basis function model}
We can create more complex models while staying in the linear framework by transforming the inputs $X$ of dimensionality $D$ through a function $\phi : D \rightarrow M$.

$y_n = \beta_0 + \sum_{i=1}^{M} \beta_i \phi_i(\bf{x_n}) =  \bf{\tilde\phi^T}(\bf{x}^T_n) \bbeta$.
The optimal $\bbeta$ can be computed in closed form by $\bbeta = ( \tilde{\bPhi}^T \tilde{\bPhi})^{-1} \tilde{\bPhi}^T y$ where $\tilde{\bPhi}$ is a matrix with N rows and the n-th row is $[1, \phi_1(\bf{x}_n),  ...,  \phi_M(\bf{x}_n)]$. But note this requires $\tilde{\bPhi}^T \tilde{\bPhi}$ to be invertible (well-conditionned: $\tilde\bPhi$ full column-rank).

Ridge regression: $\bbeta_{ridge} = ( \tilde{\boldsymbol\Phi}^T \tilde{\boldsymbol\Phi} + \lambda \boldsymbol I)^{-1} \tilde{\boldsymbol\Phi}^T \boldsymbol y$ ($\boldsymbol I$ is $M+1 \times M+1$ and $=0$ on top left). Improves condition number of Gram matrix by lifting eigenvalues.

MAP: $\bbeta_{ridge} = \arg \max_\beta \log[\prod\limits_{n=1}^N \mathcal{N}(y_n | \bf{x}_n^T\bbeta, \lambda) \times \mathcal{N}(\bbeta|0,\bf{I})]$

\subsection{Cost functions}
%\begin{colfig}
%\centering
%\includegraphics[width=\linewidth]{images/error-functions.png}
%\end{colfig}

Cost function / Loss: $\mathcal{L}(\bbeta) = \mathcal{L}(\mathcal{D},\bbeta)$

Mean square error (MSE): $\frac{1}{2N} \sum_{n=1}^{N}\left[y_n-f(\bf{x}_i) \right]^2$

MSE matrix formulation: $\frac{1}{2N} (\bf{y - X \bbeta})^T (\bf{y - X \bbeta})$

Mean absolute error (MAE): $\frac{1}{2N} \sum_{n=1}^{N}\left | y_n-f(\bf{x}_i) \right |$

Huber loss: $\mathcal{L}_\delta (a) = \begin{cases}
\frac{1}{2}{a^2}                   & \text{for } |a| \le \delta, \\
\delta (|a| - \frac{1}{2}\delta ), & \text{otherwise.}
\end{cases}$

Root mean square error (RMSE): $\sqrt{2 * \text{MSE}}$

Epsilon insensitive (``hinge loss'', used for SVM):
$\mathcal{L}_{\epsilon}(y, \hat{y}) = \begin{cases}
0                   & \text{if } |y - \hat y| \le \epsilon, \\
|y - \hat y| - \epsilon, & \text{otherwise.}
\end{cases}$

% ----------
\section{Least squares}
Complexity: $\mathcal{O}(ND^2 + D^3)$
The gradient of the MSE set to zero is called the normal equation:
$  \bf{X}^T \bf{X} \bbeta - \bf{X}^T \bf{y} = 0$.

Geometric interpretation: choosing a vector in the span of $\tilde{X}$. $\tilde{X}$ needs full rank ($=D$) $\Rightarrow$ dim. of null space 0 $\Rightarrow$ Gram matrix positive definite $\Rightarrow$ invertible.

We can solve this for $\bbeta$ directly (by matrix manipulation)
$\bbeta = ( \bf{\tilde{X}}^T \bf{\tilde{X}} )^{-1} \bf{\tilde{X}}^T \bf{y}$

Doesn't work when $\tilde{X}$ is rank deficient ($D>N$) or matrix is \textbf{ill-conditioned} (columns (nearly) colinear, rounding can cause problems). For ill-conditioned matrices we may use partial pivoting (pivoting on the largest variable available in column).

% ----------
\section{Classification}
Logistic Function: $\sigma(t) = \frac{exp(t)}{1+exp(t)}$\\
Derivative: $\frac{ \partial\sigma(t) }{ \partial t } = \sigma(t)[ 1 - \sigma(t) ]$

Classification with linear regression: Use $y = 0$ as class $\mathcal{C}_1$
and $y = 1$ as class $\mathcal{C}_2$ and then decide a newly estimated $y$ belongs
to $\mathcal{C}_1$ if $y < 0.5$.

$k$-Nearest Neighbor: $f_k(\bf{x}_*) = k^{-1} \sum_{\bf{x}_n \in nbh_k (\bf{x}_*)}$, $nbh_k (\bf{x})$ $k$ closest points of $\bf{x}$.

\subsection{Logistic Regression}
$\mathcal{L}_{mle}(\bbeta) = \sum\limits_{n=1}^N y_n \log \sigma(\tilde{\bf{x}}_n^T \bbeta) + (1-y_n) \log[1-\sigma(\tilde{\bf{x}}_n^T \bbeta)]
= \sum\limits_{n=1}^N y_n \tilde{\bf{x}}_n^T \bbeta - \log[1 + \exp(\tilde{\bf{x}}_n^T \bbeta)]$

$\frac{ \partial\mathcal{L}(\bbeta) }{ \partial \bbeta } = \tilde{\bf{X}}^T [\sigma(\tilde{\bf{X}} \bbeta) - \bf{y}]$

$\bf{H}(\bbeta) = \bf{\tilde{X}^T S \tilde{X}}, \bf{S}_{nn} = \sigma(\bf{\tilde{x}}_n^T \bbeta) [1 - \sigma(\bf{\tilde{x}}_n^T \bbeta)]$ (diag)

There's no closed form, we can use gradient descent.

Linearly separable problem has many solutions $\Rightarrow$ solved by L2 regularization (penalized log. reg.): $\min\limits_\beta -\sum\limits_{n=1}^N \log p(y_n | \bf{x}_n^T, \bbeta) + \lambda \sum\limits_{d=1}^D\beta^2_d$ or $\bf{X^T S X} + \lambda \bf{I}_{D+1}$ (top-left = 0).

\subsection{Generalized linear model}
The GLM consists of three elements: \textbf{1)} a probability distribution from the exponential family; \textbf{2)} a linear predictor $\hat y = \bf{X} \bbeta$; \textbf{3)} a link function g such that $E(\phi(y)) = \mu = g^{-1}(\eta)$.

In a GLM each outcome of the dependent variables $y$ is assumed to be generated from a particular distribution in the exponential family, e.g. normal, binomial, Poisson, gamma, etc.

ML cost $\min \mathcal{L}(\bbeta) = - \sum_{n=1}^N \log p(y_n | \eta_n)$, $p$ exp distribution, natural parameters $\eta_n = \tilde{\bf{x}}_n^T \bbeta$. Normal equation: $\frac{\partial \mathcal{L}}{\partial \bbeta} = \tilde{\bf{X}}^T [ \bf{g}^{-1}(\eta_n) - \phi(\bf{y})]$.


\subsection{Cost functions}
RMSE: $\sqrt{\frac{1}{N} \sum_{n=1}^{N}\left[y_n- \hat{p_n} \right]^2}$

0-1 Loss: $ \frac{1}{N} \sum_{n=1}^{N} \delta(y_n, \hat{y_n})$

Log-Loss: $- \frac{1}{N}  \sum_{n=1}^{N} y_n \log(\hat{p_n}) + (1-y_n) \log(1-\hat{p_n})$


% ----------
\section{Probabilistic framework}
The Likelihood Function maps the model parameters to the probability distribution of $\bf{y}$:
$\mathcal{L}_{lik}\colon \text{parameter space} \to [0;1]\quad  \bbeta \mapsto p(\bf{y} \mid  \bbeta)$
An underlying $p$ is assumed before. If the observed $y$ are IID, $p(\bf{y} \mid \bbeta) = \prod_n p(y_n \mid \bbeta)$.

$\mathcal{L}_{lik}$ can be viewed as just another cost function. Maximum likelihood then simply chooses the parameters $\bbeta$ such that observed data is most likely. $\bbeta = \argmax_{\text{all} \bbeta} L(\bbeta)$

Assuming different $p$ is basically what makes this so flexible. We can chose e.g.:

\begin{tabular}{ l  l }
  \hline
  Gaussian $p$ & $\mathcal{L}_{lik} \widehat{=} -\mathcal{L}_{MSE}$ \\
  Laplace $p$  & $\mathcal{L}_{lik} \widehat{=} -\mathcal{L}_{MAE}$ \\
  \hline
\end{tabular}

It is a sample approximation of the expected likelihood:
$\mathcal{L}_{lik}(\bbeta) \approx E_y[ p(y \mid \bbeta) ]$.
It is consistent (more data points $\Rightarrow$ converges in probability) and efficient (in best possible manner, achieves Cramor-Rao lower bound: Cov($\bbeta_{mle}$)$=\bf{I^{-1}}(\bbeta_{true})$, $\bf{I}$ is Fisher).

\subsection{Bayesian methods}
Bayes rule: $p(A, B) = p(A|B) p(B) = p(B|A) p(A)$

The \textbf{prior} $p(\bf{f}|\bf{X})$ encodes our prior belief about the ``true'' model $\bf{f}$. The \textbf{likelihood} $p(\bf{y}|\bf{f})$ measures the probability of our (possibly noisy) observations given the prior.

Parameters $\theta = \{\mu, \Sigma, \pi\} \rightarrow$ Intent variable $\bf{z} \rightarrow$ Data $\bf{y} \leftarrow \theta$: $P_{\btheta} (\bf{y}, \bf{z})$. Inference: find $\bf{z}$ given $\bf{y}, \btheta$, Learning: find $\btheta$ given $\bf{y}$.

Least-squares tries to find model parameters $\bbeta$ which maximize the likelihood. Ridge regression maximizes the \textbf{posterior} $p(\bbeta|\bf{y})$

\subsection{Bayesian networks}
Given a directed graph $G$ and parameters $\theta$, bayesian network defines joint distribution $p_{\theta}(\bf{x}) = \prod_{k=1}^K p_{\theta}(x_k | pa_k), pa_k$ parents of $x_k$. 

\textbf{Exemple}: Latent factors $z_i$ and observations results $y_i$, factorization $p(y_1, y_2, z_1, z_2, z_3) = p(y_1 | z_1, z_2) p(y_2 | z_2, z_3) p(z_1) p(z_2) p(z_3)$.

\begin{colfig}
  \centering
  \includegraphics[height=1cm]{images/bayesian-network.png}
\end{colfig}

We can then obtain the distribution over latent factors ($z_i$) by marginalizing over the unknown variables (marginal posterior probabilities):
$p(z_1, z_2, z_3 | y_1, y_2) = \frac{\text{joint}}{p(y_1, y_2)}$\\
$\implies p(z_1 | y_1, y_2) = \sum_{z_2, z_3} \frac{\text{joint}}{p(y_1, y_2)}$

Complexity for $D$ unknowns $\mathcal{O}(2^M E)$, $E$ \#edges.

\textbf{Sum-product algorithm}: $p(z_1 | \bf{y}) \propto$
$\sum_{z_3} \sum_{z_2} p(y_a|z_1,z_2) p(y_b | z_2, z_3) p(z_1) p(z_2) p(z_3
) = p(z_1) \{ \sum_{z_2} p(y_a | z_1, z_2) p(z_2) [ \sum_{z_3} p(y_b | z_2, z_3) p(z_3) ] \}$

\subsection{Belief propagation}
Learning inference of latent variables. Message-passing based algorithm used to compute desired marginals (e.g. $p(z_1 | y_1, y_2)$). $\bf{1)}$ Init all $m_i$ and iter until stabilization: $\bf{a)}$ $m_{i \rightarrow a}$, $\bf{b)}$ $m_{a \rightarrow i}$; $\bf{2)}$ get marginals $p(z_i | \bf{y}) = p(z_j) \prod_{j \in \mathbb{N} (a)} m_{j \rightarrow a} (z_j)$.

$m_{i \rightarrow a}(z_i) = p(z_i) \prod_{b \in \mathbb{N}(i) \setminus a} m_{b \rightarrow i}(z_i) $

$m_{a \rightarrow i}(z_i) = \sum_{j \neq i} p(y_a | pa_a) \prod_{j \in \mathbb{N}(a) \setminus i } m_{j \rightarrow a}(z_j)$


% ----------
\section{Kernel methods}
Kernels are another way to encode our prior believe about the relation of outputs to inputs. A kernel defines a distance measure, or ``similarity'' of two vectors. We define:
$(\bf{K})_{i,j} = \kappa(\bf{x_i}, \bf{x_j}) = \vec \phi(\bf{x_i})^T \vec \phi(\bf{x_j})$.

The $\phi$ are not that important in the end, because we only use the Kernel as is. Sometimes it's even impossible to write them down explicitly.

$k(\bf{x_i}, \bf{x_j})$: Linear $= \bf{x_i}^T \bf{x_j}$, 
Polynomial $= (\bf{x_i}^T \bf{x_j} + c)^d$,
RBF $= \exp\left(-\frac{||\bf{x_i} - \bf{x_j}||^2}{2\sigma^2}\right)$.

Properties of a Kernel: $\bf{1)}$ $\bf{K}$ should be symmetric: $\bf{K}^T = \bf{K}$; $\bf{2)}$ $\bf{K}$ should be positive semi-definite: $\forall$ nonzero vector $\bf{a}, \bf{a}^T \bf{K} \bf{a} \geq 0$.

\subsection{Primal vs. Dual}
Instead of working in the \textbf{column space} of our data, we can work in the \textbf{row space}:
$\bf{\hat{y} = X \bbeta = X X^T \balpha = K \balpha}$
where $\bbeta \in \mathbb{R}^D$ and $\balpha \in \mathbb{R}^N$
and (like magic) $\bf{K}$ shows up, the Kernel Matrix.

\textbf{Representer Theorem}: For any $\bbeta$ minimizing
$\min_\beta \sum_{n=1}^N \mathcal{L}(y_n, \bf{x}_n^T \bbeta) + \sum_{d=1}^D \lambda \beta_d^2$
there exists an $\balpha$ such that $\bbeta = \bf{X}^T \balpha$.

When we have an explicit vector formulation of $\bbeta$, we can use the matrix inversion lemma to get to the dual. E.g. for ridge regression:
$\bbeta = (\bf{X}^T \bf{X}  + \lambda \bf{I}_D)^{-1} \bf{X}^T \bf{y}= \bf{X}^T \underbrace{(\bf{X X}^T + \lambda \bf{I}_N)^{-1} \bf{y}}_{\balpha}$

Getting the dual in optimization (equivalent):
$\min_{\beta} g(\beta) \leftrightarrow 
\min_{\beta} \max_{\alpha} G(\beta, \alpha) \leftrightarrow 
\max_{\alpha} \min_{\beta} G(\beta, \alpha) \leftrightarrow 
\max_{\alpha} g^*(\alpha)$

Set $\bf{G}(\balpha, \bbeta)$: $C[v_n]_+ = \max_{\alpha_n} \alpha_n v_n, \alpha_n \in [0, C]$.
\textbf{Minimax}: switch min-max OK when $G(\balpha, \bbeta)$ convex in $\bbeta$ and concave in $\balpha$ and sets are convex.
\textbf{Get dual}: switch min/max, derive, equal to 0, plug back $\bbeta$.

\subsection{Gaussian Process}
Predicting function $\bf{f}$ interpreted as a RV with jointly gaussian prior: $\mathcal{N}(\bf{f} | \bf{0}, K(X))$.
Kernel function $k(\bf{x}_n, \bf{x}_m)$ defines the prior. Idea: if $\bf{x}_i$ and $\bf{x_j}$ are similar kernel-wise, output of points similar too.

We sample functions from RV $\bf{f}$ and use prior + measurements to generate predictions. With measurements $\bf{y}$, we get joint distribution with $\bf{\hat{f}}$ to be predicted:

$
\begin{bmatrix}
  \bf{y} \\
  \bf{\hat{f}}
\end{bmatrix}
=
\mathcal{N} \left(
  \bf{0},
  \begin{bmatrix}
    K(X, X) + \sigma_n^2 I  & K(X, \hat{X}) \\
    K(\hat{X}, X),          & K(\hat{X}, \hat{X})
  \end{bmatrix}
\right)
$


Conditioning on $\bf{y}$ (proba. output): $\bf{\hat{f}} | \bf{y}, \hat{X}, X \sim \mathcal{N}(\mu ' , \Sigma ')$ with:
$\mu' = K(\hat{X}, X) [K(X,X) + \sigma_n^2 I]^{-1} \bf{y}, \Sigma' = K(\hat{X}, \hat{X}) - K(\hat{X}, X)[K(X,X) + \sigma_n^2 I ]^{-1} K(X, \hat{X})$


% ----------
\section{Neural Networks}
A feed forward NN has $K$ layers with $\{y_n, \bf{x}_n\}$ $n$th input-output pair, $\bf{z}_n^{(k)}$ $k$th hidden vector, $\bf{a}_n^{(k)}$ corresponding activation, $g$ link function. For the $k$th layer and $m$th activation: 
$\bf{a}_n^{(k)} = \bf{B}^{(k)} \bf{z}_n^{(k-1)}, \bf{z}_n^{(k)} = h( \bf{a}_n^{(k)}), \bf{z}_n^{(0)} = \bf{x}_n, \bf{B}^{(k)} \text{ has rows } (\bbeta_m^{(k)})^T$.

$\bf{x}_n \xrightarrow{\bf{B}^{(1)}} \bf{a}_n^{(1)} \xrightarrow{h(\bf{a}_n^{(k)})} \bf{z}_n^{(1)} \xrightarrow{\bf{B}^{(2)}} \dots \bf{z}_n^{(K-1)} \xrightarrow{g} \bf{y}_n$

$\hat{y}_n = g((\bbeta^{(K-1)})^T * h(\bf{B}^{(K-2)} * h(\dots * h(\bf{B}^{(1)} * \bf{x}_n)))$.

\paragraph{Backpropagation}
Algorithm computing $\frac{\partial \mathcal{L}}{\partial \bf{B}^{(k)}}$:
$\bf{1)}$ Forward pass: compute $\bf{a}_n^{(k)}$, $\bf{z}_n^{(k)}$ and $\bf{y}_n$ from $\bf{x}_n$;
$\bf{2)}$ Backward pass: derivatives $\bdelta_n^{(k-1)} := \partial \mathcal{L} / \partial \bf{a}_n^{(k-1)} = \text{diag}[ \bf{h}'(\bf{a}^{(k)}) ] (\bf{B^{(k)}})^T \bdelta^{(k)}$;
$\bf{3)}$ $\frac{\partial \mathcal{L}}{\partial \bf{B}^{(k)}} = \sum_n \bdelta_n^{(k)} (\bf{z}_n^{(k)})^T, \frac{\partial \mathcal{L}}{\partial \bf{B}^{(1)}} = \bdelta^{(1)} \bf{x}^T, \frac{\partial \mathcal{L}}{\partial \bf{B}^{(k)}} = \bdelta^{(k)} \bf{z}^{(k)T}$.

%removed: not in slides
%\subsection{Regularization}
%NN are not \textit{identifiable} (existence of many local optima), therefore the maximum likelihood estimator is not \textit{consistent}.

%NN are universal density estimators, and thus prone to severe overfitting. Techniques used to reduce overfitting include early stopping (stop optimizing when test error starts increasing) and ``weight decay'' (i.e. $L_2$ regularization).

% ----------
\section{Support Vector Machines}
Search for the hyperplane separating the data such that the gap (margin $M = \frac{1}{||\beta||}$) is biggest.
It minimizes the following hinge loss cost function:

$\mathcal{L}_{SVM} (\bbeta)= \sum_{n=1}^N C[1 - y_n \tilde\phi_n^T \bbeta]_{+} + \frac{1}{2} \sum_{j=1}^M \beta_j^2$, with $[t]_{+} = \max(0, t)$ (or with $\lambda$ on $2^{nd}$ term). 
Convex but not differentiable. In the dual, the same problem can be formulated as:
$\max_{\alpha \in [0; C]^N} \balpha^T \bf{1} - \frac{1}{2} \balpha^T \bf{Y K Y} \balpha , \quad \balpha^T \bf{y} = 0$

$\balpha$ is sparse and non-zero only for data instrumental in determining decision boundaries. $\alpha_n$ is the slope of lines lower bound of Hinge. 3 kinds support vectors $\tilde{\phi}_n$: $\bf{1)}$ Not a SV (outside margin, $\alpha_n = 0$); $\bf{2)}$ essential SV (on the margin, $\alpha_n \in (0,C)$); $\bf{3)}$ bound SV (outside margin, other side, $\alpha_n = C$).

% ----------
\section{Trees}
Top-down building (greedy), one split at a time, recursively (space partitioning).
Splitting data: find a split $(k, \tau)$ minimizing impurity measure at leaves: $\{ k^*, \tau^* \} = \argmin_{k, \tau} I_{\text{split}} (D, k, \tau), x_k > \tau$.

Given $N$ training $X=\{\bf{x}_i, y_i\}^N_{i=1}, y_i \in \{0,1\}$:
$\bf{1)}$ split training data $(L,R) = \{\bf{x}_i, y_i\} \text{ s.t. } x_{ik} (>,\leq) \tau$
$\bf{2)}$ leaf proba. $p_{(L,R)} = \frac{\#y_i = 1}{N_{(L,R)}}$
$\bf{3)}$ impurity $I_{\text{split}} = N_L I(p_L) + N_R I(p_R)$

Impurity measures: misclassification error $I(p) = 1 - \max (p, 1-p)$, cross-entropy $I(p) = -p \log p - (1-p) \log (1-p)$, gini $I(p) = 2p(1-p)$

\subsection{Random Forests / Model Averaging}
Training: learn $M$ trees on different subsets of data; prediction: average prediction $f_i$ of each tree. We want large $M$ and low correlation between them. Var reduction ratio: $\frac{V(z_1 = f_1) = \sigma^2}{V(z_M = \frac{1}{M} \sum_{i=1}^M f_i) = \frac{1}{M} \sigma^2 + \rho \frac{M-1}{M} \sigma^2} \rightarrow \frac{1}{\rho}$

Decorrelating trees: 
$\bf{1)}$ bagging (randomize training data): generate $X^i$ by randomly sampling $N$ samples with replacement from $X$.
$\bf{2)}$ randomize feature space: search for $k$ over reduced random subset of features $m_{\text{try}}$.

Parameters: max tree depth, nb of trees, $m_{\text{try}} (= \sqrt{D}$).

% ----------
\section{Unsupervised Learning}
\subsection{K-Means}
$\min_{\bf{r,\mu}} \mathcal{L}(\bf{r, \mu}) = 
\sum_{k=1}^K \sum_{n=1}^N r_{nk} ||\bf{x}_n - \bf{\mu}_k ||_2^2, \mu_k \in \mathbb{R}$ centroids, $r_{nk} \in \{0,1\}$  cluster assignments, $\sum_{k=1}^K r_{nk} = 1$.
\textbf{Optimization}: init $\mu_k \forall k$ and iter: $\bf{1)}$ $\bf{r}_n \forall n = 1$ if closest, $0$ o/w, given $\mu$, 
$\bf{2)}$ $\mu_k \forall k = 
\frac{\sum^N_{n=1} r_{nk} \bf{x}_n}{\sum^N_{n=1} r_{nk}}$
given $\bf{r}$.
Convergence assured. Complexity $\mathcal{O}(NKDI)$, $I =$ number of iterations.

\subsection{Gaussian Mixture Models}
$K$-Means extension where examples can belong to several elliptic clusters, using full covariance matrix $\Sigma_k$. $r_n, p(r_n=k)=\pi_k, \sum \pi_k = 1$ multinomial distribution.
\textbf{Model} $p(\bf{X}, \bf{r} | \{\bf{\mu}, \bf{\Sigma}, \bf{\pi}\} = \bf{\theta}) = \prod\limits_{n=1}^N \prod\limits_{k=1}^K [\{\mathcal{N}(\bf{x}_n | \bf{\mu}_k , \bf{\Sigma}_k ) \}^{r_{nk}}] \prod\limits_{k=1}^K [\pi_k]^{r_{nk}}$

\textbf{Marginal likelihood} (because latent variable $r_n$): $p(\bf{x}_n | \bf{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\bf{x}_n | \bf{\mu}_k, \bf{\Sigma}_k)$

\textbf{Maximum likelihood} (cost, not identifiable nor convex): $\max_{\bf{\theta}} \sum_{n=1}^N \log \sum_{k=1}^K \pi_k \mathcal{N} ( \bf{x}_n | \bf{\mu}_k , \bf{\Sigma}_k)$

To use this for clustering, we first fit this mixture and then compute the posterior $p(z_i = k | \bf{x}_i, \theta)$. This yields \textit{soft} cluster assignments.

\subsection{Matrix Factorization}

Project $\bf{x}_n$ to smaller dimension $\bf{z}_n \in \mathbb{R}^M$ to minimize reconstruction error (convex, not identifiable, [regularization]): $\min_{\bf{W, Z}} \frac{1}{2} \sum_{n=1}^N \sum_{d=1}^D (x_{dn} - \bf{w}_d^T \bf{z}_n )^2 
+ [\frac{\lambda_w}{2} \sum_{d=1}^D \bf{w}_d^T \bf{w}_d + \frac{\lambda_z}{2} \sum_{n=1}^N \bf{z}_n^T \bf{z}_n], 
\bf{X}: D \times N, \bf{W}: D \times M, \bf{Z}: N \times M$

\textbf{Alternating Least-Squares} $\mathcal{O}(DM^2 + M^3) (D \gg M)$: $\bf{Z}^T \leftarrow (\bf{W}^T \bf{W} + \lambda_z \bf{I}_M )^{-1} \bf{W}^T \bf{X},
\bf{W}^T \leftarrow (\bf{Z}^T \bf{Z} + \lambda_w \bf{I}_M )^{-1} \bf{Z}^T \bf{X}^T$

% ----------
\section{PCA}
Dimensionality reduction, latent factor model, minimizes reconstruction error / maximizes variance of projection / decorrelate data. In matrix factorization we get $\bf{\tilde{X}} = \bf{WZ}^T$, if columns of $\bf{W}$ are orthogonal, we have PCA.

Find the eigenvectors of the covariance matrix $\bf{X^T X}$ of the data. These form an orthonormal basis $\{ \bf{w}_1, ..., \bf{w}_N\}$ for the data in the directions that have highest variance.
One can then use the first $L < D$ vectors to rebuild the data: $\bf{\hat{x}}_i = \bf{W} \bf{z}_i = \bf{W} \bf{W}^T \bf{x}_i$, with $\bf{W} = \begin{bmatrix} \bf{w}_1 ; ... ; \bf{w}_L \end{bmatrix}$.
This minimizes mean square error $\frac{1}{N} \sum_{i=1}^N \bf{x}_i - \bf{\hat{x}}_i^2$.

Principal components (columns of $\bf{U}$) decorrelate the cov matrix: $\bf{U}^T \bf{XX}^T \bf{U} = \bf{S}^2$.

\subsection{SVD}
Get orthogonal factorization $\bf{X} = \bf{USV}^T$, $\bf{U}, \bf{V}$ left and right singular vectors (orthonormal of sizes $D \times D, N \times N$), $\bf{S}$ singular values (diagonal ordered values size $D \times N$). Approximations $\bf{W} = \bf{US}^{1/2}, \bf{Z} = \bf{VS}^{1/2}$.

Spectral view $\bf{X} = \sum_{j=1}^D s_j \bf{u}_j \bf{v}_j^T$:
\begin{colfig}
  \centering
  \includegraphics[height=2cm]{images/svd.png}
\end{colfig}

The singular vals of a $N \times D$ matrix $\bf{X}$ are the square roots of the eigenvalues of the $D \times D$ matrix $\bf{X^T X}$


% ----------
\section{Concepts}

\subsection{Convexity}
f is called convex f: $\forall x_1, x_2 \in X, \forall t \in [0, 1]: \qquad f(tx_1+(1-t)x_2)\leq t f(x_1)+(1-t)f(x_2).$

Sum of two convex functions is convex. Composition of a convex function with a convex, nondecreasing function is convex. Linear, exponential and $\log(\sum \exp)$ functions are convex.

\subsection{Cross-Validation}
$K$-Fold cross-validation: split training data in $K$ splits, choose $1$ as test, $K-1$ as training, repeat and average test and training error. We can get estimate of generalization error, assess quality of model, get best parameters.

\subsection{Bias-Variance Decomposition}
\begin{colfig}
  \centering
  \includegraphics[trim=0cm 3cm 0cm 0cm, clip=true, height=3cm]{images/bias-variance.png}
\end{colfig}

In-sample error: error of one test sample. Test error: average over test data of in-sample error. Expected test error $\overline{teErr}$: average over all training data of test error. 

Bias-variance comes directly out of the test error ($y_{true}=E[y]:$ true model, $(x,y = y_{true} + \epsilon):$ test pair observation, $\hat{y}=f(\bf{x}):$ test predictions):
$\overline{teErr} 
= E[(\text{observation} - \text{prediction})^2] = E[(y - \hat{y})^2]
= E[(y - y_{true} + y_{true} - \hat{y})^2]
=\underbrace{E[(y - y_{true})^2]}_{\text{var of measurement}} + E[(y_{true} - \hat{y})^2]
=\sigma^2 + E[(y_{true} - E[\hat{y}] + E[\hat{y}] - \hat{y})^2]
=\sigma^2 + \underbrace{E[(y_{true} - E[\hat{y}])^2]}_{\text{pred bias}^2} +\underbrace{E[(E[\hat{y}] - \hat{y})^2]}_{\text{pred variance}}
$

Adjustment methods (bias/variance): regularization (+/-), choose simpler model (+/-), more data (-/ ).

\subsection{Identifiability}
Statistical model $\mathcal{P} = \{P_\theta: \theta \in \Theta\}$ identifiable if mapping $\theta \mapsto P_\theta$ is one-to-one:
$P_{\theta_1}=P_{\theta_2} \quad\Rightarrow\quad \theta_1=\theta_2 \quad\ \forall \theta_1,\theta_2\in\Theta.$
A non-identifiable model will typically have many local optima yielding the same cost, e.g. $\mathcal{L}(W, Z) = \mathcal{L}(aW, \frac{1}{a} Z)$

\subsection{Curse of dimensionality}
Dimensionality increase $\Rightarrow$ all points spread $\Rightarrow$ NN choice becomes random. 
In high dimension, data only covers a tiny fraction of the input space, making generalization difficult and increasing bias and variance.
Expected edge length $e_D(r) = r^{1/D}$: to capture $r$ of data we must cover $e_D(r)$ of input variable range $\Rightarrow$ sampling density proportional to $N^{1/D}$.



% ----- Less useful concepts


\subsection{Occam's Razor}
It states that among competing hypotheses, the one with the fewest assumptions should be selected. Other, more complicated solutions may ultimately prove correct, but in the absence of certainty, the fewer assumptions that are made, the better.

\todo[inline]{TODO: statistical goodness (robust to outliers, ...) vs. computational goodness (convex, low computational complexity, ...) tradeoff. No free lunch theorem.}

% ---------- Credits
\section{Credits}
Most material was taken from the lecture notes of \href{http://people.epfl.ch/228491}{Prof. Emtiyaz Khan}.\\
Cost functions figure from Patrick Breheny's slides.\\
Biais-variance decomposition figure from Hastie, Tibshirani, Friedman, \textit{The Elements of Statistical Learning}.
The SVD figure from Kevin P. Murphy, \textit{Machine Learning, A Probabilistic Perspective}.

% ---------- Footer
\hrule
\tiny
Rendered \today. Written by Dennis Meier and Merlin Nimier-David.
\copyright Dennis Meier. This work is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License.
To view a copy of this license, visit \href{http://creativecommons.org/licenses/by-sa/3.0/}{http://creativecommons.org/licenses/by-sa/3.0/} or
send a letter to Creative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.
\includegraphics{images/by-sa.png}

\end{multicols*}
\end{document}
